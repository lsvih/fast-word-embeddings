from nltk.tokenize import sent_tokenize, word_tokenize


def tokenize_string(string):
    """

    :param string: e.g. 'salt lake city'. In the most general case, the string will be a long series of sentences.
    :return: list of tokens
    """
    list_of_sentences = list()
    tmp = list()
    tmp.append(string)
    k = list()
    k.append(tmp)
    # print k
    list_of_sentences += k  # we are assuming this is a unicode/string

    word_tokens = list()
    for sentences in list_of_sentences:
        for sentence in sentences:
            for s in sent_tokenize(sentence):
                word_tokens += word_tokenize(s)
    return word_tokens


def preprocess_tokens(tokens_list, options=['remove_non_alpha', 'lower']):
    """

    :param tokens_list: The list generated by tokenize_field per object
    :param options: A list of to-dos.
    :return: A list of processed tokens. The original list is unmodified.
    """
    new_list = list(tokens_list)
    for option in options:
        if option == 'remove_non_alpha':
            tmp_list = list()
            for token in new_list:
                if token.isalpha():
                    tmp_list.append(token)
            del new_list
            new_list = tmp_list
        elif option == 'lower':
            for i in range(0, len(new_list)):
                new_list[i] = new_list[i].lower()
        else:
            print 'Warning. Option not recognized: ' + option

    return new_list


def tokenize_field(obj, field):
    """
    At present, we'll deal with only one field (e.g. readability_text). The field could be a unicode
    or a list, so make sure to take both into account.

    We are not preprocessing the tokens in any way. For this, I'll write another function.
    :param obj: the adultservice json object
    :param field: e.g. 'readability_text'
    :return: A list of tokens.
    """
    list_of_sentences = list()

    # print obj['readability_text']
    if field not in obj:
        return None
    elif type(obj[field]) == list:
        k = list()
        k.append(obj[field])
        list_of_sentences += k
    else:
        tmp = list()
        tmp.append(obj[field])
        k = list()
        k.append(tmp)
        # print k
        list_of_sentences += k  # we are assuming this is a unicode/string

    word_tokens = list()
    for sentences in list_of_sentences:
        # print sentences
        for sentence in sentences:
            for s in sent_tokenize(sentence):
                word_tokens += word_tokenize(s)

    return word_tokens


def is_sublist_in_big_list(big_list, sublist):
    # matches = []
    for i in range(len(big_list)):
        if big_list[i] == sublist[0] and big_list[i:i + len(sublist)] == sublist:
            return True
    return False